{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412b3157",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682196258878,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "412b3157"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a26e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fc1c74",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682196262173,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "43fc1c74"
   },
   "outputs": [],
   "source": [
    "def puf_query(c, w):\n",
    "    n = c.shape[1]\n",
    "    phi = np.ones(n+1)\n",
    "    phi[n] = 1\n",
    "    for i in range(n-1, -1, -1):\n",
    "        phi[i] = (2*c[0,i]-1)*phi[i+1]\n",
    "\n",
    "    r = (np.dot(phi, w) > 0)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8895a54a",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682196264307,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "8895a54a"
   },
   "outputs": [],
   "source": [
    "# Problem Setup\n",
    "target = 0.99  # The desired prediction rate\n",
    "n = 64  # number of stages in the PUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e49db4",
   "metadata": {
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1682196277720,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "20e49db4"
   },
   "outputs": [],
   "source": [
    "# Initialize the PUF\n",
    "np.random.seed(int(time.time()))\n",
    "data = np.loadtxt('weight_diff.txt')\n",
    "w = np.zeros((n+1, 1))\n",
    "for i in range(1, n+2):\n",
    "    randi_offset = np.random.randint(1, 45481)\n",
    "    w[i-1] = data[randi_offset-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35286d48",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1682196278261,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "35286d48"
   },
   "outputs": [],
   "source": [
    "# Syntax to query the PUF:\n",
    "c = np.random.randint(0, 2, size=(1, n))  # a random challenge vector\n",
    "r = puf_query(c, w)\n",
    "# you may remove these two lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af1fd6c5",
   "metadata": {
    "executionInfo": {
     "elapsed": 3362,
     "status": "ok",
     "timestamp": 1682196283779,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "af1fd6c5"
   },
   "outputs": [],
   "source": [
    "# Generating the training set #ENTER TRAINING SIZE HERE FOR DIFFERENT TRAILS\n",
    "training_size = 5000  # Number of training samples\n",
    "X_train = np.random.randint(0, 2, size=(training_size, n))  # Random challenge vectors\n",
    "y_train = np.zeros((training_size, 1))  # Response bits\n",
    "for i in range(training_size):\n",
    "    y_train[i] = puf_query(X_train[i].reshape(1, -1), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09f27b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1682190650627,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "b09f27b8",
    "outputId": "1a803836-3e48-47aa-eedc-2828759b209a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea63027d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1682190653757,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "ea63027d",
    "outputId": "3bb489f6-e199-4558-ab6e-63448dbb2544"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7dde546",
   "metadata": {
    "executionInfo": {
     "elapsed": 6008,
     "status": "ok",
     "timestamp": 1682196290307,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "c7dde546"
   },
   "outputs": [],
   "source": [
    "# Calculating Phi\n",
    "def calc_phi(select_bits):\n",
    "    phi_vals = []\n",
    "    for i in range(len(select_bits)):\n",
    "        target_slice = select_bits[i:]\n",
    "        zeros = [z for z in target_slice if z == 0]\n",
    "        phi = 1 if len(zeros) % 2 == 0 else -1\n",
    "        phi_vals.append(phi)\n",
    "    return np.array(phi_vals + [1])\n",
    "\n",
    "\n",
    "X_train_phi = np.apply_along_axis(calc_phi, 1, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01033dca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1682190666825,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "01033dca",
    "outputId": "dc245edb-47b2-44b5-e3c5-a774e2c72230"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1, ..., -1,  1,  1],\n",
       "       [ 1,  1,  1, ..., -1, -1,  1],\n",
       "       [-1,  1,  1, ...,  1, -1,  1],\n",
       "       ...,\n",
       "       [ 1, -1,  1, ...,  1, -1,  1],\n",
       "       [-1, -1,  1, ..., -1, -1,  1],\n",
       "       [-1,  1, -1, ...,  1,  1,  1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edaa93e1",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1682196290308,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "edaa93e1"
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return torch.sigmoid(torch.tensor(z)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gFf0ULvomvB0",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682196290309,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "gFf0ULvomvB0"
   },
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost(phi, theta, y):\n",
    "    z = phi @ theta\n",
    "  \n",
    "    epsilon =  0.001\n",
    "    cost0 = y.T.dot(np.log(sigmoid(z) + epsilon))\n",
    "    cost1 = (1-y).T.dot(np.log(1-sigmoid(z) + epsilon))\n",
    "    cost = -((cost1 + cost0))/len(y) \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c03b809",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682196308934,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "8c03b809"
   },
   "outputs": [],
   "source": [
    "# Logistic regression #ADJUST LEARNING RATE AND ITERATIONS FOR EACH TRAIL (THIS CELL AND CELL BELOW)\n",
    "def logistic_regression(X, y, learning_rate=0.01, num_iterations=500):\n",
    "    m, n = X.shape\n",
    "    w = np.random.randn(n)\n",
    "    w = w.reshape(-1, 1)\n",
    "    phi = X\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        loss = cost(phi, w, y)\n",
    "        z = np.dot(phi, w)\n",
    "        h = sigmoid(z)\n",
    "  \n",
    "        np.dot(phi.T, h - np.reshape(y,(len(y),1)))\n",
    "        w = w - learning_rate * np.dot(phi.T, sigmoid(np.dot(phi, w)) - np.reshape(y,(len(y),1)))\n",
    "\n",
    "        print(\"Iteration {}/{} - Loss: {}\".format(i+1, num_iterations, loss))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f9ec66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12090,
     "status": "ok",
     "timestamp": 1682196360691,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "54f9ec66",
    "outputId": "0a703702-3098-4838-d6d1-190ef9612e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/500 - Loss: [[2.74279196]]\n",
      "Iteration 2/500 - Loss: [[0.75511166]]\n",
      "Iteration 3/500 - Loss: [[0.11056461]]\n",
      "Iteration 4/500 - Loss: [[0.03410826]]\n",
      "Iteration 5/500 - Loss: [[0.02535872]]\n",
      "Iteration 6/500 - Loss: [[0.02357001]]\n",
      "Iteration 7/500 - Loss: [[0.02304507]]\n",
      "Iteration 8/500 - Loss: [[0.0228508]]\n",
      "Iteration 9/500 - Loss: [[0.022753]]\n",
      "Iteration 10/500 - Loss: [[0.02268678]]\n",
      "Iteration 11/500 - Loss: [[0.02263204]]\n",
      "Iteration 12/500 - Loss: [[0.0225819]]\n",
      "Iteration 13/500 - Loss: [[0.02253381]]\n",
      "Iteration 14/500 - Loss: [[0.02248678]]\n",
      "Iteration 15/500 - Loss: [[0.0224404]]\n",
      "Iteration 16/500 - Loss: [[0.02239449]]\n",
      "Iteration 17/500 - Loss: [[0.02234898]]\n",
      "Iteration 18/500 - Loss: [[0.02230383]]\n",
      "Iteration 19/500 - Loss: [[0.02225901]]\n",
      "Iteration 20/500 - Loss: [[0.02221453]]\n",
      "Iteration 21/500 - Loss: [[0.02217036]]\n",
      "Iteration 22/500 - Loss: [[0.02212651]]\n",
      "Iteration 23/500 - Loss: [[0.02208297]]\n",
      "Iteration 24/500 - Loss: [[0.02203974]]\n",
      "Iteration 25/500 - Loss: [[0.02199681]]\n",
      "Iteration 26/500 - Loss: [[0.02195418]]\n",
      "Iteration 27/500 - Loss: [[0.02191184]]\n",
      "Iteration 28/500 - Loss: [[0.0218698]]\n",
      "Iteration 29/500 - Loss: [[0.02182804]]\n",
      "Iteration 30/500 - Loss: [[0.02178657]]\n",
      "Iteration 31/500 - Loss: [[0.02174538]]\n",
      "Iteration 32/500 - Loss: [[0.02170447]]\n",
      "Iteration 33/500 - Loss: [[0.02166383]]\n",
      "Iteration 34/500 - Loss: [[0.02162346]]\n",
      "Iteration 35/500 - Loss: [[0.02158336]]\n",
      "Iteration 36/500 - Loss: [[0.02154352]]\n",
      "Iteration 37/500 - Loss: [[0.02150394]]\n",
      "Iteration 38/500 - Loss: [[0.02146463]]\n",
      "Iteration 39/500 - Loss: [[0.02142557]]\n",
      "Iteration 40/500 - Loss: [[0.02138676]]\n",
      "Iteration 41/500 - Loss: [[0.0213482]]\n",
      "Iteration 42/500 - Loss: [[0.02130989]]\n",
      "Iteration 43/500 - Loss: [[0.02127182]]\n",
      "Iteration 44/500 - Loss: [[0.021234]]\n",
      "Iteration 45/500 - Loss: [[0.02119641]]\n",
      "Iteration 46/500 - Loss: [[0.02115906]]\n",
      "Iteration 47/500 - Loss: [[0.02112194]]\n",
      "Iteration 48/500 - Loss: [[0.02108506]]\n",
      "Iteration 49/500 - Loss: [[0.0210484]]\n",
      "Iteration 50/500 - Loss: [[0.02101197]]\n",
      "Iteration 51/500 - Loss: [[0.02097576]]\n",
      "Iteration 52/500 - Loss: [[0.02093977]]\n",
      "Iteration 53/500 - Loss: [[0.020904]]\n",
      "Iteration 54/500 - Loss: [[0.02086844]]\n",
      "Iteration 55/500 - Loss: [[0.0208331]]\n",
      "Iteration 56/500 - Loss: [[0.02079798]]\n",
      "Iteration 57/500 - Loss: [[0.02076306]]\n",
      "Iteration 58/500 - Loss: [[0.02072835]]\n",
      "Iteration 59/500 - Loss: [[0.02069384]]\n",
      "Iteration 60/500 - Loss: [[0.02065954]]\n",
      "Iteration 61/500 - Loss: [[0.02062543]]\n",
      "Iteration 62/500 - Loss: [[0.02059153]]\n",
      "Iteration 63/500 - Loss: [[0.02055782]]\n",
      "Iteration 64/500 - Loss: [[0.02052431]]\n",
      "Iteration 65/500 - Loss: [[0.02049099]]\n",
      "Iteration 66/500 - Loss: [[0.02045786]]\n",
      "Iteration 67/500 - Loss: [[0.02042492]]\n",
      "Iteration 68/500 - Loss: [[0.02039216]]\n",
      "Iteration 69/500 - Loss: [[0.02035959]]\n",
      "Iteration 70/500 - Loss: [[0.02032721]]\n",
      "Iteration 71/500 - Loss: [[0.020295]]\n",
      "Iteration 72/500 - Loss: [[0.02026298]]\n",
      "Iteration 73/500 - Loss: [[0.02023113]]\n",
      "Iteration 74/500 - Loss: [[0.02019946]]\n",
      "Iteration 75/500 - Loss: [[0.02016796]]\n",
      "Iteration 76/500 - Loss: [[0.02013664]]\n",
      "Iteration 77/500 - Loss: [[0.02010548]]\n",
      "Iteration 78/500 - Loss: [[0.0200745]]\n",
      "Iteration 79/500 - Loss: [[0.02004368]]\n",
      "Iteration 80/500 - Loss: [[0.02001303]]\n",
      "Iteration 81/500 - Loss: [[0.01998254]]\n",
      "Iteration 82/500 - Loss: [[0.01995222]]\n",
      "Iteration 83/500 - Loss: [[0.01992206]]\n",
      "Iteration 84/500 - Loss: [[0.01989205]]\n",
      "Iteration 85/500 - Loss: [[0.01986221]]\n",
      "Iteration 86/500 - Loss: [[0.01983252]]\n",
      "Iteration 87/500 - Loss: [[0.01980299]]\n",
      "Iteration 88/500 - Loss: [[0.01977361]]\n",
      "Iteration 89/500 - Loss: [[0.01974439]]\n",
      "Iteration 90/500 - Loss: [[0.01971531]]\n",
      "Iteration 91/500 - Loss: [[0.01968639]]\n",
      "Iteration 92/500 - Loss: [[0.01965761]]\n",
      "Iteration 93/500 - Loss: [[0.01962899]]\n",
      "Iteration 94/500 - Loss: [[0.0196005]]\n",
      "Iteration 95/500 - Loss: [[0.01957216]]\n",
      "Iteration 96/500 - Loss: [[0.01954397]]\n",
      "Iteration 97/500 - Loss: [[0.01951591]]\n",
      "Iteration 98/500 - Loss: [[0.019488]]\n",
      "Iteration 99/500 - Loss: [[0.01946023]]\n",
      "Iteration 100/500 - Loss: [[0.01943259]]\n",
      "Iteration 101/500 - Loss: [[0.01940509]]\n",
      "Iteration 102/500 - Loss: [[0.01937773]]\n",
      "Iteration 103/500 - Loss: [[0.0193505]]\n",
      "Iteration 104/500 - Loss: [[0.0193234]]\n",
      "Iteration 105/500 - Loss: [[0.01929644]]\n",
      "Iteration 106/500 - Loss: [[0.01926961]]\n",
      "Iteration 107/500 - Loss: [[0.01924291]]\n",
      "Iteration 108/500 - Loss: [[0.01921633]]\n",
      "Iteration 109/500 - Loss: [[0.01918989]]\n",
      "Iteration 110/500 - Loss: [[0.01916357]]\n",
      "Iteration 111/500 - Loss: [[0.01913737]]\n",
      "Iteration 112/500 - Loss: [[0.0191113]]\n",
      "Iteration 113/500 - Loss: [[0.01908536]]\n",
      "Iteration 114/500 - Loss: [[0.01905953]]\n",
      "Iteration 115/500 - Loss: [[0.01903383]]\n",
      "Iteration 116/500 - Loss: [[0.01900825]]\n",
      "Iteration 117/500 - Loss: [[0.01898279]]\n",
      "Iteration 118/500 - Loss: [[0.01895744]]\n",
      "Iteration 119/500 - Loss: [[0.01893222]]\n",
      "Iteration 120/500 - Loss: [[0.01890711]]\n",
      "Iteration 121/500 - Loss: [[0.01888211]]\n",
      "Iteration 122/500 - Loss: [[0.01885723]]\n",
      "Iteration 123/500 - Loss: [[0.01883247]]\n",
      "Iteration 124/500 - Loss: [[0.01880781]]\n",
      "Iteration 125/500 - Loss: [[0.01878327]]\n",
      "Iteration 126/500 - Loss: [[0.01875884]]\n",
      "Iteration 127/500 - Loss: [[0.01873452]]\n",
      "Iteration 128/500 - Loss: [[0.01871031]]\n",
      "Iteration 129/500 - Loss: [[0.0186862]]\n",
      "Iteration 130/500 - Loss: [[0.01866221]]\n",
      "Iteration 131/500 - Loss: [[0.01863832]]\n",
      "Iteration 132/500 - Loss: [[0.01861453]]\n",
      "Iteration 133/500 - Loss: [[0.01859085]]\n",
      "Iteration 134/500 - Loss: [[0.01856728]]\n",
      "Iteration 135/500 - Loss: [[0.01854381]]\n",
      "Iteration 136/500 - Loss: [[0.01852044]]\n",
      "Iteration 137/500 - Loss: [[0.01849717]]\n",
      "Iteration 138/500 - Loss: [[0.018474]]\n",
      "Iteration 139/500 - Loss: [[0.01845094]]\n",
      "Iteration 140/500 - Loss: [[0.01842797]]\n",
      "Iteration 141/500 - Loss: [[0.0184051]]\n",
      "Iteration 142/500 - Loss: [[0.01838233]]\n",
      "Iteration 143/500 - Loss: [[0.01835966]]\n",
      "Iteration 144/500 - Loss: [[0.01833708]]\n",
      "Iteration 145/500 - Loss: [[0.0183146]]\n",
      "Iteration 146/500 - Loss: [[0.01829221]]\n",
      "Iteration 147/500 - Loss: [[0.01826992]]\n",
      "Iteration 148/500 - Loss: [[0.01824772]]\n",
      "Iteration 149/500 - Loss: [[0.01822561]]\n",
      "Iteration 150/500 - Loss: [[0.0182036]]\n",
      "Iteration 151/500 - Loss: [[0.01818168]]\n",
      "Iteration 152/500 - Loss: [[0.01815985]]\n",
      "Iteration 153/500 - Loss: [[0.0181381]]\n",
      "Iteration 154/500 - Loss: [[0.01811645]]\n",
      "Iteration 155/500 - Loss: [[0.01809489]]\n",
      "Iteration 156/500 - Loss: [[0.01807341]]\n",
      "Iteration 157/500 - Loss: [[0.01805203]]\n",
      "Iteration 158/500 - Loss: [[0.01803072]]\n",
      "Iteration 159/500 - Loss: [[0.01800951]]\n",
      "Iteration 160/500 - Loss: [[0.01798838]]\n",
      "Iteration 161/500 - Loss: [[0.01796734]]\n",
      "Iteration 162/500 - Loss: [[0.01794638]]\n",
      "Iteration 163/500 - Loss: [[0.0179255]]\n",
      "Iteration 164/500 - Loss: [[0.01790471]]\n",
      "Iteration 165/500 - Loss: [[0.017884]]\n",
      "Iteration 166/500 - Loss: [[0.01786337]]\n",
      "Iteration 167/500 - Loss: [[0.01784283]]\n",
      "Iteration 168/500 - Loss: [[0.01782236]]\n",
      "Iteration 169/500 - Loss: [[0.01780198]]\n",
      "Iteration 170/500 - Loss: [[0.01778167]]\n",
      "Iteration 171/500 - Loss: [[0.01776145]]\n",
      "Iteration 172/500 - Loss: [[0.0177413]]\n",
      "Iteration 173/500 - Loss: [[0.01772123]]\n",
      "Iteration 174/500 - Loss: [[0.01770124]]\n",
      "Iteration 175/500 - Loss: [[0.01768133]]\n",
      "Iteration 176/500 - Loss: [[0.01766149]]\n",
      "Iteration 177/500 - Loss: [[0.01764173]]\n",
      "Iteration 178/500 - Loss: [[0.01762204]]\n",
      "Iteration 179/500 - Loss: [[0.01760243]]\n",
      "Iteration 180/500 - Loss: [[0.0175829]]\n",
      "Iteration 181/500 - Loss: [[0.01756343]]\n",
      "Iteration 182/500 - Loss: [[0.01754404]]\n",
      "Iteration 183/500 - Loss: [[0.01752473]]\n",
      "Iteration 184/500 - Loss: [[0.01750549]]\n",
      "Iteration 185/500 - Loss: [[0.01748631]]\n",
      "Iteration 186/500 - Loss: [[0.01746722]]\n",
      "Iteration 187/500 - Loss: [[0.01744819]]\n",
      "Iteration 188/500 - Loss: [[0.01742923]]\n",
      "Iteration 189/500 - Loss: [[0.01741034]]\n",
      "Iteration 190/500 - Loss: [[0.01739152]]\n",
      "Iteration 191/500 - Loss: [[0.01737277]]\n",
      "Iteration 192/500 - Loss: [[0.01735409]]\n",
      "Iteration 193/500 - Loss: [[0.01733548]]\n",
      "Iteration 194/500 - Loss: [[0.01731694]]\n",
      "Iteration 195/500 - Loss: [[0.01729846]]\n",
      "Iteration 196/500 - Loss: [[0.01728005]]\n",
      "Iteration 197/500 - Loss: [[0.01726171]]\n",
      "Iteration 198/500 - Loss: [[0.01724344]]\n",
      "Iteration 199/500 - Loss: [[0.01722523]]\n",
      "Iteration 200/500 - Loss: [[0.01720708]]\n",
      "Iteration 201/500 - Loss: [[0.017189]]\n",
      "Iteration 202/500 - Loss: [[0.01717098]]\n",
      "Iteration 203/500 - Loss: [[0.01715303]]\n",
      "Iteration 204/500 - Loss: [[0.01713514]]\n",
      "Iteration 205/500 - Loss: [[0.01711732]]\n",
      "Iteration 206/500 - Loss: [[0.01709956]]\n",
      "Iteration 207/500 - Loss: [[0.01708186]]\n",
      "Iteration 208/500 - Loss: [[0.01706422]]\n",
      "Iteration 209/500 - Loss: [[0.01704665]]\n",
      "Iteration 210/500 - Loss: [[0.01702913]]\n",
      "Iteration 211/500 - Loss: [[0.01701168]]\n",
      "Iteration 212/500 - Loss: [[0.01699429]]\n",
      "Iteration 213/500 - Loss: [[0.01697696]]\n",
      "Iteration 214/500 - Loss: [[0.01695968]]\n",
      "Iteration 215/500 - Loss: [[0.01694247]]\n",
      "Iteration 216/500 - Loss: [[0.01692532]]\n",
      "Iteration 217/500 - Loss: [[0.01690822]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 218/500 - Loss: [[0.01689119]]\n",
      "Iteration 219/500 - Loss: [[0.01687421]]\n",
      "Iteration 220/500 - Loss: [[0.01685729]]\n",
      "Iteration 221/500 - Loss: [[0.01684042]]\n",
      "Iteration 222/500 - Loss: [[0.01682362]]\n",
      "Iteration 223/500 - Loss: [[0.01680687]]\n",
      "Iteration 224/500 - Loss: [[0.01679018]]\n",
      "Iteration 225/500 - Loss: [[0.01677354]]\n",
      "Iteration 226/500 - Loss: [[0.01675696]]\n",
      "Iteration 227/500 - Loss: [[0.01674043]]\n",
      "Iteration 228/500 - Loss: [[0.01672396]]\n",
      "Iteration 229/500 - Loss: [[0.01670754]]\n",
      "Iteration 230/500 - Loss: [[0.01669118]]\n",
      "Iteration 231/500 - Loss: [[0.01667487]]\n",
      "Iteration 232/500 - Loss: [[0.01665862]]\n",
      "Iteration 233/500 - Loss: [[0.01664242]]\n",
      "Iteration 234/500 - Loss: [[0.01662627]]\n",
      "Iteration 235/500 - Loss: [[0.01661018]]\n",
      "Iteration 236/500 - Loss: [[0.01659413]]\n",
      "Iteration 237/500 - Loss: [[0.01657814]]\n",
      "Iteration 238/500 - Loss: [[0.0165622]]\n",
      "Iteration 239/500 - Loss: [[0.01654632]]\n",
      "Iteration 240/500 - Loss: [[0.01653048]]\n",
      "Iteration 241/500 - Loss: [[0.0165147]]\n",
      "Iteration 242/500 - Loss: [[0.01649896]]\n",
      "Iteration 243/500 - Loss: [[0.01648328]]\n",
      "Iteration 244/500 - Loss: [[0.01646765]]\n",
      "Iteration 245/500 - Loss: [[0.01645206]]\n",
      "Iteration 246/500 - Loss: [[0.01643653]]\n",
      "Iteration 247/500 - Loss: [[0.01642105]]\n",
      "Iteration 248/500 - Loss: [[0.01640561]]\n",
      "Iteration 249/500 - Loss: [[0.01639022]]\n",
      "Iteration 250/500 - Loss: [[0.01637489]]\n",
      "Iteration 251/500 - Loss: [[0.0163596]]\n",
      "Iteration 252/500 - Loss: [[0.01634435]]\n",
      "Iteration 253/500 - Loss: [[0.01632916]]\n",
      "Iteration 254/500 - Loss: [[0.01631401]]\n",
      "Iteration 255/500 - Loss: [[0.01629891]]\n",
      "Iteration 256/500 - Loss: [[0.01628386]]\n",
      "Iteration 257/500 - Loss: [[0.01626885]]\n",
      "Iteration 258/500 - Loss: [[0.01625389]]\n",
      "Iteration 259/500 - Loss: [[0.01623898]]\n",
      "Iteration 260/500 - Loss: [[0.01622411]]\n",
      "Iteration 261/500 - Loss: [[0.01620929]]\n",
      "Iteration 262/500 - Loss: [[0.01619451]]\n",
      "Iteration 263/500 - Loss: [[0.01617978]]\n",
      "Iteration 264/500 - Loss: [[0.0161651]]\n",
      "Iteration 265/500 - Loss: [[0.01615045]]\n",
      "Iteration 266/500 - Loss: [[0.01613586]]\n",
      "Iteration 267/500 - Loss: [[0.0161213]]\n",
      "Iteration 268/500 - Loss: [[0.01610679]]\n",
      "Iteration 269/500 - Loss: [[0.01609233]]\n",
      "Iteration 270/500 - Loss: [[0.0160779]]\n",
      "Iteration 271/500 - Loss: [[0.01606352]]\n",
      "Iteration 272/500 - Loss: [[0.01604919]]\n",
      "Iteration 273/500 - Loss: [[0.01603489]]\n",
      "Iteration 274/500 - Loss: [[0.01602064]]\n",
      "Iteration 275/500 - Loss: [[0.01600643]]\n",
      "Iteration 276/500 - Loss: [[0.01599227]]\n",
      "Iteration 277/500 - Loss: [[0.01597814]]\n",
      "Iteration 278/500 - Loss: [[0.01596406]]\n",
      "Iteration 279/500 - Loss: [[0.01595002]]\n",
      "Iteration 280/500 - Loss: [[0.01593602]]\n",
      "Iteration 281/500 - Loss: [[0.01592206]]\n",
      "Iteration 282/500 - Loss: [[0.01590814]]\n",
      "Iteration 283/500 - Loss: [[0.01589426]]\n",
      "Iteration 284/500 - Loss: [[0.01588042]]\n",
      "Iteration 285/500 - Loss: [[0.01586663]]\n",
      "Iteration 286/500 - Loss: [[0.01585287]]\n",
      "Iteration 287/500 - Loss: [[0.01583915]]\n",
      "Iteration 288/500 - Loss: [[0.01582547]]\n",
      "Iteration 289/500 - Loss: [[0.01581183]]\n",
      "Iteration 290/500 - Loss: [[0.01579823]]\n",
      "Iteration 291/500 - Loss: [[0.01578467]]\n",
      "Iteration 292/500 - Loss: [[0.01577115]]\n",
      "Iteration 293/500 - Loss: [[0.01575767]]\n",
      "Iteration 294/500 - Loss: [[0.01574422]]\n",
      "Iteration 295/500 - Loss: [[0.01573082]]\n",
      "Iteration 296/500 - Loss: [[0.01571745]]\n",
      "Iteration 297/500 - Loss: [[0.01570412]]\n",
      "Iteration 298/500 - Loss: [[0.01569082]]\n",
      "Iteration 299/500 - Loss: [[0.01567757]]\n",
      "Iteration 300/500 - Loss: [[0.01566435]]\n",
      "Iteration 301/500 - Loss: [[0.01565117]]\n",
      "Iteration 302/500 - Loss: [[0.01563802]]\n",
      "Iteration 303/500 - Loss: [[0.01562491]]\n",
      "Iteration 304/500 - Loss: [[0.01561184]]\n",
      "Iteration 305/500 - Loss: [[0.0155988]]\n",
      "Iteration 306/500 - Loss: [[0.01558581]]\n",
      "Iteration 307/500 - Loss: [[0.01557284]]\n",
      "Iteration 308/500 - Loss: [[0.01555991]]\n",
      "Iteration 309/500 - Loss: [[0.01554702]]\n",
      "Iteration 310/500 - Loss: [[0.01553417]]\n",
      "Iteration 311/500 - Loss: [[0.01552134]]\n",
      "Iteration 312/500 - Loss: [[0.01550856]]\n",
      "Iteration 313/500 - Loss: [[0.01549581]]\n",
      "Iteration 314/500 - Loss: [[0.01548309]]\n",
      "Iteration 315/500 - Loss: [[0.01547041]]\n",
      "Iteration 316/500 - Loss: [[0.01545776]]\n",
      "Iteration 317/500 - Loss: [[0.01544515]]\n",
      "Iteration 318/500 - Loss: [[0.01543257]]\n",
      "Iteration 319/500 - Loss: [[0.01542002]]\n",
      "Iteration 320/500 - Loss: [[0.01540751]]\n",
      "Iteration 321/500 - Loss: [[0.01539503]]\n",
      "Iteration 322/500 - Loss: [[0.01538259]]\n",
      "Iteration 323/500 - Loss: [[0.01537018]]\n",
      "Iteration 324/500 - Loss: [[0.0153578]]\n",
      "Iteration 325/500 - Loss: [[0.01534546]]\n",
      "Iteration 326/500 - Loss: [[0.01533314]]\n",
      "Iteration 327/500 - Loss: [[0.01532086]]\n",
      "Iteration 328/500 - Loss: [[0.01530862]]\n",
      "Iteration 329/500 - Loss: [[0.0152964]]\n",
      "Iteration 330/500 - Loss: [[0.01528422]]\n",
      "Iteration 331/500 - Loss: [[0.01527207]]\n",
      "Iteration 332/500 - Loss: [[0.01525995]]\n",
      "Iteration 333/500 - Loss: [[0.01524786]]\n",
      "Iteration 334/500 - Loss: [[0.01523581]]\n",
      "Iteration 335/500 - Loss: [[0.01522378]]\n",
      "Iteration 336/500 - Loss: [[0.01521179]]\n",
      "Iteration 337/500 - Loss: [[0.01519983]]\n",
      "Iteration 338/500 - Loss: [[0.0151879]]\n",
      "Iteration 339/500 - Loss: [[0.015176]]\n",
      "Iteration 340/500 - Loss: [[0.01516413]]\n",
      "Iteration 341/500 - Loss: [[0.01515229]]\n",
      "Iteration 342/500 - Loss: [[0.01514049]]\n",
      "Iteration 343/500 - Loss: [[0.01512871]]\n",
      "Iteration 344/500 - Loss: [[0.01511696]]\n",
      "Iteration 345/500 - Loss: [[0.01510525]]\n",
      "Iteration 346/500 - Loss: [[0.01509356]]\n",
      "Iteration 347/500 - Loss: [[0.0150819]]\n",
      "Iteration 348/500 - Loss: [[0.01507027]]\n",
      "Iteration 349/500 - Loss: [[0.01505868]]\n",
      "Iteration 350/500 - Loss: [[0.01504711]]\n",
      "Iteration 351/500 - Loss: [[0.01503557]]\n",
      "Iteration 352/500 - Loss: [[0.01502406]]\n",
      "Iteration 353/500 - Loss: [[0.01501258]]\n",
      "Iteration 354/500 - Loss: [[0.01500112]]\n",
      "Iteration 355/500 - Loss: [[0.0149897]]\n",
      "Iteration 356/500 - Loss: [[0.01497831]]\n",
      "Iteration 357/500 - Loss: [[0.01496694]]\n",
      "Iteration 358/500 - Loss: [[0.0149556]]\n",
      "Iteration 359/500 - Loss: [[0.01494429]]\n",
      "Iteration 360/500 - Loss: [[0.01493301]]\n",
      "Iteration 361/500 - Loss: [[0.01492176]]\n",
      "Iteration 362/500 - Loss: [[0.01491053]]\n",
      "Iteration 363/500 - Loss: [[0.01489933]]\n",
      "Iteration 364/500 - Loss: [[0.01488816]]\n",
      "Iteration 365/500 - Loss: [[0.01487702]]\n",
      "Iteration 366/500 - Loss: [[0.0148659]]\n",
      "Iteration 367/500 - Loss: [[0.01485481]]\n",
      "Iteration 368/500 - Loss: [[0.01484375]]\n",
      "Iteration 369/500 - Loss: [[0.01483272]]\n",
      "Iteration 370/500 - Loss: [[0.01482171]]\n",
      "Iteration 371/500 - Loss: [[0.01481073]]\n",
      "Iteration 372/500 - Loss: [[0.01479977]]\n",
      "Iteration 373/500 - Loss: [[0.01478885]]\n",
      "Iteration 374/500 - Loss: [[0.01477795]]\n",
      "Iteration 375/500 - Loss: [[0.01476707]]\n",
      "Iteration 376/500 - Loss: [[0.01475622]]\n",
      "Iteration 377/500 - Loss: [[0.0147454]]\n",
      "Iteration 378/500 - Loss: [[0.0147346]]\n",
      "Iteration 379/500 - Loss: [[0.01472383]]\n",
      "Iteration 380/500 - Loss: [[0.01471309]]\n",
      "Iteration 381/500 - Loss: [[0.01470237]]\n",
      "Iteration 382/500 - Loss: [[0.01469167]]\n",
      "Iteration 383/500 - Loss: [[0.01468101]]\n",
      "Iteration 384/500 - Loss: [[0.01467036]]\n",
      "Iteration 385/500 - Loss: [[0.01465975]]\n",
      "Iteration 386/500 - Loss: [[0.01464915]]\n",
      "Iteration 387/500 - Loss: [[0.01463859]]\n",
      "Iteration 388/500 - Loss: [[0.01462804]]\n",
      "Iteration 389/500 - Loss: [[0.01461753]]\n",
      "Iteration 390/500 - Loss: [[0.01460703]]\n",
      "Iteration 391/500 - Loss: [[0.01459657]]\n",
      "Iteration 392/500 - Loss: [[0.01458612]]\n",
      "Iteration 393/500 - Loss: [[0.0145757]]\n",
      "Iteration 394/500 - Loss: [[0.01456531]]\n",
      "Iteration 395/500 - Loss: [[0.01455494]]\n",
      "Iteration 396/500 - Loss: [[0.01454459]]\n",
      "Iteration 397/500 - Loss: [[0.01453427]]\n",
      "Iteration 398/500 - Loss: [[0.01452397]]\n",
      "Iteration 399/500 - Loss: [[0.01451369]]\n",
      "Iteration 400/500 - Loss: [[0.01450344]]\n",
      "Iteration 401/500 - Loss: [[0.01449321]]\n",
      "Iteration 402/500 - Loss: [[0.01448301]]\n",
      "Iteration 403/500 - Loss: [[0.01447283]]\n",
      "Iteration 404/500 - Loss: [[0.01446267]]\n",
      "Iteration 405/500 - Loss: [[0.01445254]]\n",
      "Iteration 406/500 - Loss: [[0.01444243]]\n",
      "Iteration 407/500 - Loss: [[0.01443234]]\n",
      "Iteration 408/500 - Loss: [[0.01442227]]\n",
      "Iteration 409/500 - Loss: [[0.01441223]]\n",
      "Iteration 410/500 - Loss: [[0.01440221]]\n",
      "Iteration 411/500 - Loss: [[0.01439221]]\n",
      "Iteration 412/500 - Loss: [[0.01438224]]\n",
      "Iteration 413/500 - Loss: [[0.01437229]]\n",
      "Iteration 414/500 - Loss: [[0.01436236]]\n",
      "Iteration 415/500 - Loss: [[0.01435245]]\n",
      "Iteration 416/500 - Loss: [[0.01434257]]\n",
      "Iteration 417/500 - Loss: [[0.0143327]]\n",
      "Iteration 418/500 - Loss: [[0.01432286]]\n",
      "Iteration 419/500 - Loss: [[0.01431305]]\n",
      "Iteration 420/500 - Loss: [[0.01430325]]\n",
      "Iteration 421/500 - Loss: [[0.01429347]]\n",
      "Iteration 422/500 - Loss: [[0.01428372]]\n",
      "Iteration 423/500 - Loss: [[0.01427399]]\n",
      "Iteration 424/500 - Loss: [[0.01426428]]\n",
      "Iteration 425/500 - Loss: [[0.01425459]]\n",
      "Iteration 426/500 - Loss: [[0.01424492]]\n",
      "Iteration 427/500 - Loss: [[0.01423528]]\n",
      "Iteration 428/500 - Loss: [[0.01422565]]\n",
      "Iteration 429/500 - Loss: [[0.01421605]]\n",
      "Iteration 430/500 - Loss: [[0.01420647]]\n",
      "Iteration 431/500 - Loss: [[0.01419691]]\n",
      "Iteration 432/500 - Loss: [[0.01418736]]\n",
      "Iteration 433/500 - Loss: [[0.01417784]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 434/500 - Loss: [[0.01416835]]\n",
      "Iteration 435/500 - Loss: [[0.01415887]]\n",
      "Iteration 436/500 - Loss: [[0.01414941]]\n",
      "Iteration 437/500 - Loss: [[0.01413997]]\n",
      "Iteration 438/500 - Loss: [[0.01413055]]\n",
      "Iteration 439/500 - Loss: [[0.01412116]]\n",
      "Iteration 440/500 - Loss: [[0.01411178]]\n",
      "Iteration 441/500 - Loss: [[0.01410242]]\n",
      "Iteration 442/500 - Loss: [[0.01409309]]\n",
      "Iteration 443/500 - Loss: [[0.01408377]]\n",
      "Iteration 444/500 - Loss: [[0.01407448]]\n",
      "Iteration 445/500 - Loss: [[0.0140652]]\n",
      "Iteration 446/500 - Loss: [[0.01405594]]\n",
      "Iteration 447/500 - Loss: [[0.01404671]]\n",
      "Iteration 448/500 - Loss: [[0.01403749]]\n",
      "Iteration 449/500 - Loss: [[0.01402829]]\n",
      "Iteration 450/500 - Loss: [[0.01401911]]\n",
      "Iteration 451/500 - Loss: [[0.01400995]]\n",
      "Iteration 452/500 - Loss: [[0.01400081]]\n",
      "Iteration 453/500 - Loss: [[0.01399169]]\n",
      "Iteration 454/500 - Loss: [[0.01398259]]\n",
      "Iteration 455/500 - Loss: [[0.01397351]]\n",
      "Iteration 456/500 - Loss: [[0.01396445]]\n",
      "Iteration 457/500 - Loss: [[0.0139554]]\n",
      "Iteration 458/500 - Loss: [[0.01394638]]\n",
      "Iteration 459/500 - Loss: [[0.01393737]]\n",
      "Iteration 460/500 - Loss: [[0.01392839]]\n",
      "Iteration 461/500 - Loss: [[0.01391942]]\n",
      "Iteration 462/500 - Loss: [[0.01391047]]\n",
      "Iteration 463/500 - Loss: [[0.01390154]]\n",
      "Iteration 464/500 - Loss: [[0.01389262]]\n",
      "Iteration 465/500 - Loss: [[0.01388373]]\n",
      "Iteration 466/500 - Loss: [[0.01387485]]\n",
      "Iteration 467/500 - Loss: [[0.01386599]]\n",
      "Iteration 468/500 - Loss: [[0.01385716]]\n",
      "Iteration 469/500 - Loss: [[0.01384833]]\n",
      "Iteration 470/500 - Loss: [[0.01383953]]\n",
      "Iteration 471/500 - Loss: [[0.01383075]]\n",
      "Iteration 472/500 - Loss: [[0.01382198]]\n",
      "Iteration 473/500 - Loss: [[0.01381323]]\n",
      "Iteration 474/500 - Loss: [[0.0138045]]\n",
      "Iteration 475/500 - Loss: [[0.01379578]]\n",
      "Iteration 476/500 - Loss: [[0.01378709]]\n",
      "Iteration 477/500 - Loss: [[0.01377841]]\n",
      "Iteration 478/500 - Loss: [[0.01376975]]\n",
      "Iteration 479/500 - Loss: [[0.01376111]]\n",
      "Iteration 480/500 - Loss: [[0.01375248]]\n",
      "Iteration 481/500 - Loss: [[0.01374387]]\n",
      "Iteration 482/500 - Loss: [[0.01373528]]\n",
      "Iteration 483/500 - Loss: [[0.01372671]]\n",
      "Iteration 484/500 - Loss: [[0.01371815]]\n",
      "Iteration 485/500 - Loss: [[0.01370961]]\n",
      "Iteration 486/500 - Loss: [[0.01370109]]\n",
      "Iteration 487/500 - Loss: [[0.01369258]]\n",
      "Iteration 488/500 - Loss: [[0.01368409]]\n",
      "Iteration 489/500 - Loss: [[0.01367562]]\n",
      "Iteration 490/500 - Loss: [[0.01366717]]\n",
      "Iteration 491/500 - Loss: [[0.01365873]]\n",
      "Iteration 492/500 - Loss: [[0.01365031]]\n",
      "Iteration 493/500 - Loss: [[0.01364191]]\n",
      "Iteration 494/500 - Loss: [[0.01363352]]\n",
      "Iteration 495/500 - Loss: [[0.01362515]]\n",
      "Iteration 496/500 - Loss: [[0.01361679]]\n",
      "Iteration 497/500 - Loss: [[0.01360845]]\n",
      "Iteration 498/500 - Loss: [[0.01360013]]\n",
      "Iteration 499/500 - Loss: [[0.01359183]]\n",
      "Iteration 500/500 - Loss: [[0.01358354]]\n",
      "Training time: 0.046875\n",
      "Training size: 5000\n"
     ]
    }
   ],
   "source": [
    "# Estimating w and timed trials\n",
    "w0 = np.zeros((n+1, 1))  # The estimated value of w.\n",
    "t0 = time.process_time()\n",
    "w0 = logistic_regression(X_train_phi, y_train, learning_rate=0.01, num_iterations=500)\n",
    "w0 = w0.reshape(-1)\n",
    "t1 = time.process_time()\n",
    "training_time = t1 - t0  # time taken to get w0\n",
    "print(\"Training time:\", training_time)\n",
    "print(\"Training size:\", training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9NKGkKCnqoVO",
   "metadata": {
    "id": "9NKGkKCnqoVO"
   },
   "outputs": [],
   "source": [
    "w0 = w0.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "u5dwxX2CqphZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682196229388,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "u5dwxX2CqphZ",
    "outputId": "6525a0bf-35b3-4760-a1bd-e6091c15a4ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ELD35PVptWas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7302,
     "status": "ok",
     "timestamp": 1682196377091,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "ELD35PVptWas",
    "outputId": "41217663-0d5c-4a19-cc55-11e8a5a9d9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: [0.9927]\n",
      "Effective training time: 0.046875\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "n_test = 10000\n",
    "correct = 0\n",
    "for i in range(1, n_test+1):\n",
    "    c_test = np.random.randint(0, 2, size=(1, n))  # a random challenge vector\n",
    "    r = puf_query(c_test, w)\n",
    "    r0 = puf_query(c_test, w0)\n",
    "    correct += (r==r0)\n",
    "\n",
    "success_rate = correct/n_test\n",
    "print(\"Success rate:\", success_rate)\n",
    "\n",
    "# If the success rate is less than 99%, a penalty time will be added\n",
    "# One second is add for each 0.01% below 99%.\n",
    "effective_training_time = training_time\n",
    "if success_rate < 0.99:\n",
    "    effective_training_time = training_time + 10000*(0.99-success_rate)\n",
    "print(\"Effective training time:\", effective_training_time)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
