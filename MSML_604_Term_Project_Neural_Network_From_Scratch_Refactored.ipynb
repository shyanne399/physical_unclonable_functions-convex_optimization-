{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1683237960770,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "h2KIx9Bhohsc",
    "outputId": "8346d686-dd1c-44b9-e2af-a5610981e8b3"
   },
   "outputs": [],
   "source": [
    "# !wget \"https://raw.githubusercontent.com/00ber/ml-projects/main/data/weight_diff.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683237961262,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "8zitPx6WJ_LM"
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "\n",
    "# Custom Binary Cross Entropy Loss \n",
    "class BCELossValue:\n",
    "  def __init__(self, y_pred, y_true, value):\n",
    "    self.y_pred = y_pred \n",
    "    self.y_true = y_true\n",
    "    self.value = value\n",
    "   \n",
    "  def backward(self, epsilon=1e-12, next_grad=1):\n",
    "    z = np.concatenate([self.y_true, self.y_pred], axis=1)\n",
    "    def calc_loss(y_row):\n",
    "      y_tr, y_pr = y_row\n",
    "      if y_tr == 1:\n",
    "        return -1/(y_pr + epsilon)\n",
    "      elif y_tr == 0:\n",
    "        return 1/(1 - y_pr + epsilon)\n",
    "      else:\n",
    "        raise Exception(\"Invalid y_pred value\")\n",
    "\n",
    "    val = np.apply_along_axis(calc_loss, 1, z)\n",
    "    val = val.reshape(-1, 1)\n",
    "    return val\n",
    "\n",
    "# Custom Sigmoid activation\n",
    "class SigmoidValue:\n",
    "  def __init__(self, x, value):\n",
    "    self.x = x \n",
    "    self.value = value \n",
    "\n",
    "  def backward(self, loss_grad):\n",
    "    return loss_grad * (self.value * (1 - self.value))\n",
    "\n",
    "  def __sub__(self, other):\n",
    "    return self.value - other\n",
    "\n",
    "  def __rsub__(self, other):\n",
    "    return other - self.value \n",
    "\n",
    "  def __truediv__(self, other):\n",
    "    return self.value / other \n",
    "\n",
    "  def ___rtruediv__(self, other):\n",
    "    return other / self.value\n",
    "\n",
    "def sigmoid_fn(x):\n",
    "    # value = 1/(1 + np.exp(-1 * x))\n",
    "    # Using scipy for sigmoid because np.exp can't calculate for larger values \n",
    "    value = scipy.special.expit(x)\n",
    "    return SigmoidValue(x, value)\n",
    "\n",
    "def bce_loss(y_pred, y_true, epsilon=1e-12):\n",
    "    value = -y_true * np.log(y_pred.value + epsilon) - (1 - y_true) * np.log(1 - y_pred.value + epsilon)\n",
    "    return BCELossValue(y_pred.value, y_true, value)\n",
    "\n",
    "class PUF:\n",
    "  def __init__(self , n, low=-10, high=10):\n",
    "    self.n = n\n",
    "    self.weight = np.random.uniform(low=low, high=high, size=(n + 1, 1))\n",
    "    self._backward = lambda: None\n",
    "\n",
    "\n",
    "  def __call__(self, phis):\n",
    "    out = phis @ self.weight\n",
    "    return sigmoid_fn(out)\n",
    "\n",
    "  def _calculate_gradients(self, phis, activation_grad):\n",
    "    return activation_grad * phis\n",
    "\n",
    "  def backward(self, phi, logits, loss, learning_rate):\n",
    "      loss_gradient = loss.backward()\n",
    "      activation_gradient = logits.backward(loss_gradient)\n",
    "      batch_gradient = self._calculate_gradients(phi, activation_gradient)\n",
    "      avg_gradient = np.mean(batch_gradient, axis=0).reshape(self.weight.shape)\n",
    "      self.update(avg_gradient, learning_rate)\n",
    "    \n",
    "  def parameters(self):\n",
    "    return self.weight\n",
    "\n",
    "  def update(self, gradient, learning_rate):\n",
    "    self.weight += -1 * learning_rate * gradient\n",
    "\n",
    "\n",
    "# Function to train the model using the provided hyperparameters\n",
    "def train(model, num_epochs, lr, X_train, X_test, y_train, y_test):\n",
    "\n",
    "  for k in range(num_epochs):\n",
    "      avg_training_loss = 0.0\n",
    "      total_training_loss = 0.0\n",
    "      avg_val_loss = 0.0\n",
    "      total_val_loss = 0.0\n",
    "      \n",
    "      \n",
    "      ypred = model(X_train)\n",
    "      loss = bce_loss(ypred, y_train)\n",
    "      model.backward(X_train, ypred, loss, lr)\n",
    "      total_training_loss += np.mean(loss.value)  \n",
    "      avg_training_loss = total_training_loss\n",
    "\n",
    "      \n",
    "      ypred = model(X_test)\n",
    "      loss = bce_loss(ypred, y_test)\n",
    "      total_val_loss += np.mean(loss.value)  \n",
    "      avg_val_loss = total_val_loss\n",
    "      avg_val_loss = total_val_loss\n",
    "      print(f\"[{k}/{num_epochs}] Avg Training Loss: {avg_training_loss} Avg Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14467,
     "status": "ok",
     "timestamp": 1683237975724,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "kLLPBKwOvvRF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def puf_query(c, w):\n",
    "    n = c.shape[1]\n",
    "    phi = np.ones(n+1)\n",
    "    phi[n] = 1\n",
    "    for i in range(n-1, -1, -1):\n",
    "        phi[i] = (2*c[0,i]-1)*phi[i+1]\n",
    "\n",
    "    r = (np.dot(phi, w) > 0)\n",
    "    return r\n",
    "    \n",
    "# Problem Setup\n",
    "target = 0.99  # The desired prediction rate\n",
    "n = 64  # number of stages in the PUF\n",
    "\n",
    "# Initialize the PUF\n",
    "np.random.seed(int(time.time()))\n",
    "data = np.loadtxt('./weight_diff.txt')\n",
    "w = np.zeros((n+1, 1))\n",
    "for i in range(1, n+2):\n",
    "    randi_offset = np.random.randint(1, 45481)\n",
    "    w[i-1] = data[randi_offset-1]\n",
    "\n",
    "# Syntax to query the PUF:\n",
    "c = np.random.randint(0, 2, size=(1, n))  # a random challenge vector\n",
    "r = puf_query(c, w)\n",
    "# you may remove these two lines\n",
    "\n",
    "# You can use the puf_query function to generate your training dataset\n",
    "# ADD YOUR DATASET GENERATION CODE HERE\n",
    "training_size = 15000\n",
    "X = np.random.randint(0, 2, size=(training_size, n))\n",
    "y = np.zeros((training_size, 1))\n",
    "\n",
    "for i in range(training_size):\n",
    "  y[i] = puf_query(X[i].reshape(1, -1), w)\n",
    "\n",
    "\n",
    "def calc_phi(select_bits):\n",
    "    phi_vals = []\n",
    "    for i in range(len(select_bits)):\n",
    "      target_slice = select_bits[i:]\n",
    "      zeros = [z for z in target_slice if z == 0]\n",
    "      phi = 1 if len(zeros) % 2 == 0 else -1\n",
    "      phi_vals.append(phi)\n",
    "    return np.array(phi_vals + [1])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train_phi = np.apply_along_axis(calc_phi, 1, X_train)\n",
    "X_test_phi = np.apply_along_axis(calc_phi, 1, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1683237975725,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "HdNvzX1orQ1C"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import math \n",
    "import scipy\n",
    "\n",
    "def train_and_eval(params):\n",
    "    lr = params[\"lr\"]\n",
    "    num_epochs = params[\"num_epochs\"] \n",
    "    weight_init = params[\"weight_init\"]\n",
    "    training_size = params[\"training_size\"] \n",
    "    results = []\n",
    "  \n",
    "    \n",
    "    n = 64\n",
    "\n",
    "    w0 = np.zeros((n+1, 1))  # The estimated value of w.\n",
    "    # Try to estimate the value of w here. This section will be timed. You are\n",
    "    # allowed to use the puf_query function here too, but it will count towards\n",
    "    # the training time.\n",
    "\n",
    "\n",
    "    t0 = time.process_time()\n",
    "    # ADD YOUR TRAINING CODE HERE\n",
    "\n",
    "    model = PUF(n, weight_init[0], weight_init[1])\n",
    "\n",
    "    train(model, num_epochs, lr, X_train_phi[:training_size], X_test_phi, y_train[:training_size], y_test)\n",
    "\n",
    "    t1 = time.process_time()\n",
    "    training_time = t1 - t0  # time taken to get w0\n",
    "    print(\"Training time:\", training_time)\n",
    "    print(\"Training size:\", training_size)\n",
    "\n",
    "\n",
    "    old_w0 = w0\n",
    "\n",
    "    wt = model.weight\n",
    "    # wt = state[\"fc.weight\"]\n",
    "    w0 = wt\n",
    "\n",
    "    # Evaluate your result\n",
    "    n_test = 10000\n",
    "    correct = 0\n",
    "    for i in range(1, n_test+1):\n",
    "        c_test = np.random.randint(0, 2, size=(1, n))  # a random challenge vector\n",
    "        r = puf_query(c_test, w)\n",
    "        r0 = puf_query(c_test, w0)\n",
    "        correct += (r==r0)\n",
    "\n",
    "    success_rate = correct/n_test\n",
    "    print(\"Success rate:\", success_rate)\n",
    "\n",
    "\n",
    "    # If the success rate is less than 99%, a penalty time will be added\n",
    "    # One second is add for each 0.01% below 99%.\n",
    "    effective_training_time = training_time\n",
    "    if success_rate < 0.99:\n",
    "        effective_training_time = training_time + 10000*(0.99-success_rate)\n",
    "    print(\"Effective training time:\", effective_training_time)\n",
    "    results.append({\n",
    "        \"lr\": lr,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"weight_init\": weight_init,\n",
    "        \"training_size\": training_size,\n",
    "        \"training_time\": training_time,\n",
    "        \"effective_training_time\": effective_training_time[0] if isinstance(effective_training_time, np.ndarray) else effective_training_time,\n",
    "        \"success_rate\": success_rate[0]\n",
    "    })\n",
    "    return results\n",
    "# Best so far\n",
    "# lr = 200\n",
    "# num_epochs = 15\n",
    "# model = PUF(64, -1, 1)\n",
    "# params_grid = [\n",
    "#     {\n",
    "#       \"lr\": 400,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (-5, 5),\n",
    "#       \"training_size\": 15000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 400,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (-5, 5),\n",
    "#       \"training_size\": 10000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 400,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (-5, 5),\n",
    "#       \"training_size\": 5000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 200,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (-1, 1),\n",
    "#       \"training_size\": 10000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 100,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (0, 1),\n",
    "#       \"training_size\": 10000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 100,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (0, 1),\n",
    "#       \"training_size\": 10000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 450,\n",
    "#       \"num_epochs\": 10,\n",
    "#       \"weight_init\": (-3, -3),\n",
    "#       \"training_size\": 5000,\n",
    "#       \"num_runs\": 1\n",
    "#     },\n",
    "#     {\n",
    "#       \"lr\": 450,\n",
    "#       \"num_epochs\": 15,\n",
    "#       \"weight_init\": (-3, -3),\n",
    "#       \"training_size\": 5000,\n",
    "#       \"num_runs\": 1\n",
    "#     }\n",
    "# ]\n",
    "# results = []\n",
    "# for params in params_grid:\n",
    "#   res = train_and_eval(params)\n",
    "#   for r in res:\n",
    "#     results.append(r)\n",
    "\n",
    "# import pandas as pd\n",
    "# results_df = pd.DataFrame.from_dict(results)\n",
    "# pd.options.display.max_rows = 4000\n",
    "# results_df.sort_values(by=[\"success_rate\"], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8376,
     "status": "ok",
     "timestamp": 1683238000888,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "U0fnyh325n26",
    "outputId": "71197d03-d910-45f5-8757-aff1207794f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/15] Avg Training Loss: 8.099580490706863 Avg Validation Loss: 2.822096050132197\n",
      "[1/15] Avg Training Loss: 2.9201081300257394 Avg Validation Loss: 1.234371239518243\n",
      "[2/15] Avg Training Loss: 1.2777795384823702 Avg Validation Loss: 0.2227728133351448\n",
      "[3/15] Avg Training Loss: 0.18530654586846346 Avg Validation Loss: 0.035335182530625356\n",
      "[4/15] Avg Training Loss: 0.026916971808450603 Avg Validation Loss: 0.013621370645761426\n",
      "[5/15] Avg Training Loss: 0.012567150541124884 Avg Validation Loss: 0.014581354705462646\n",
      "[6/15] Avg Training Loss: 0.010681492313461164 Avg Validation Loss: 0.01112626039918257\n",
      "[7/15] Avg Training Loss: 0.010617946935645366 Avg Validation Loss: 0.015817046210566642\n",
      "[8/15] Avg Training Loss: 0.011272271445776737 Avg Validation Loss: 0.01162059700705467\n",
      "[9/15] Avg Training Loss: 0.01149126922553693 Avg Validation Loss: 0.016486096451498383\n",
      "[10/15] Avg Training Loss: 0.012137177127790054 Avg Validation Loss: 0.01206820318138368\n",
      "[11/15] Avg Training Loss: 0.012402203918376735 Avg Validation Loss: 0.016924311282775193\n",
      "[12/15] Avg Training Loss: 0.012677141302009108 Avg Validation Loss: 0.012305174172825143\n",
      "[13/15] Avg Training Loss: 0.012849265181696327 Avg Validation Loss: 0.017099891052348\n",
      "[14/15] Avg Training Loss: 0.01285512676746433 Avg Validation Loss: 0.012386411704046934\n",
      "Training time: 0.015625\n",
      "Training size: 10000\n",
      "Success rate: [0.9935]\n",
      "Effective training time: 0.015625\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "  \"lr\": 400,\n",
    "  \"num_epochs\": 15,\n",
    "  \"weight_init\": (-5, 5),\n",
    "  \"training_size\": 10000,\n",
    "  \"num_runs\": 1\n",
    "}\n",
    "results = train_and_eval(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1683237984709,
     "user": {
      "displayName": "Sushant Karki",
      "userId": "10159480926888285206"
     },
     "user_tz": 240
    },
    "id": "QpCvKI0Jwm2P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cpu]",
   "language": "python",
   "name": "conda-env-pytorch_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
